# EORA AI Assistant

## Описание проекта

Проект представляет собой сервис для поиска и ответа на вопросы на основе контента веб-страниц. Основная идея — использовать LLM (Large Language Model) для формирования ответов на основе релевантных блоков текста с сайтов, одновременно указывая источники информации.

Функциональность включает:

- Получение текста веб-страниц и его очистку от шума (меню, футер, реклама, дубли, юридические блоки).
- Сохранение очищенных страниц в базу данных PostgreSQL с проверкой изменений через hash чтобы лишний раз не делать запрос для парсинга.
- Индексацию блоков текста с помощью embedding-моделей для поиска релевантных фрагментов.
- Генерацию ответов на вопросы с использованием LLM, где ответ строится исключительно на найденных блоках.
- Попытка добавления ссылок на испточники, но не уверен что сделано корректно.

Проект написан на **Python**, использует **FastAPI** для API, **SQLAlchemy** для работы с БД и **asyncio** для асинхронного выполнения.

---

## Что пробовали сделать

- Парсим страницу с помощью синхронной библиотеки
- Сделали поиск релевантных блоков с использованием embedding и similarity search.
- Настроили LLM (через gigachat) для генерации ответов строго по контексту.
- Сделали систему ссылок на источники, чтобы можно было указать, откуда информация в ответе.
- Добавили асинхронную обработку запросов и вынесли потенциально блокирующие операции в `run_in_threadpool`.

---

## Что сработало, а что не очень

**Сработало:**
- Очистка контента от шума и дублирования.
- Индексация блоков текста и поиск релевантных фрагментов.
- Генерация ответов LLM строго по контексту.
- Автоматическая генерация ссылок на источники.
- Асинхронность работы API с обработкой нескольких запросов одновременно.

**Что не идеально:**
- Изначально данных было много и сделал разбиение текста на блоки, но затем текст уменьшили оставив только полезную информацию и по факту это не пригодилось, только время потратил.
- Само разбиение текста происходит по определенному количеству слов, что не сильно правильно как я думаю. Может потеряться какая мысль или еще что-то.
- Парсер можно было бы написать на асинхронной библиотеке
- Можно было бы добавить celery например и запускать парсинг страниц раз в месяц например
- Ручки написать не функциями, а создать класс для этого
- Проект можно было бы засунуть в контейнер например
---

## Как оценивали качество решения

- По началу вообще ничего не работало так как текст был не оцищенный еще и разбитый по кускам
- Проверяли корректность нумерации и ссылки на источники.
- Оценивали скорость обработки запросов и стабильность API при одновременных запросах.

---

## Что можно добавить при большем времени

- Полное разделение страниц на блоки с секциями (статьи, инструкции, отзывы).
- Более точная фильтрация дубликатов и шумного текста.
- Автоматическое тестирование качества ответов на заранее подготовленных вопросах.
- Оптимизация обработки очень больших текстов и их chunking для LLM.
- Логирование, метрики и мониторинг использования модели и качества ответов.
- Фронтенд для визуального тестирования и взаимодействия с сервисом.

---

## Настройка проекта

### 1. Установка зависимостей

Создаём и активируем виртуальное окружение:

```bash
Устанавливаем виртуальное окружение
python -m venv venv

активируем его
source venv/bin/activate  # Linux/macOS
или
source venv\Scripts\activate     # Windows

Устанавливаем зависимости
pip install -r requirements.txt

Скачиваем образ и создаем контейнер с БД(postgresql)
docker-compose up -d

Применяем миграции алембика
alembic upgrade head

Устанавливаем расширение
docker exec -it eora_db psql -U eora -d eora -c "CREATE EXTENSION IF NOT EXISTS vector;"

Запускаем веб сервер
uvicorn app.main:app --reload

Документация будет по ссылке
http://127.0.0.1:8000/docs
```
